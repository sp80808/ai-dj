import json
import time
import gc
from llama_cpp import Llama


class DJAILL:
    """Interface avec le LLM qui joue le rÃ´le du DJ"""

    def __init__(self, model_path, config=None):
        """
        Initialise l'interface avec le LLM

        Args:
            model_path (str): Chemin vers le modÃ¨le GMA-4B
            config (dict): Configuration globale
        """
        self.model_path = model_path
        self.session_state = config or {}

        # Historique de conversation pour le LLM
        self.conversation_history = [
            {"role": "system", "content": self.get_system_prompt()}
        ]

    def destroy_model(self):
        if hasattr(self, "model"):
            try:
                del self.model
                gc.collect()
                print("ğŸ§¹ ModÃ¨le dÃ©truit")
            except Exception as e:
                print(f"âš ï¸ Erreur lors de la destruction du modÃ¨le: {e}")

    def init_model(self):
        """Initialise ou rÃ©initialise le modÃ¨le LLM"""
        print(f"âš¡ Initialisation du modÃ¨le LLM depuis {self.model_path}...")

        # Initialiser un nouveau modÃ¨le LLM
        self.model = Llama(
            model_path=self.model_path,
            n_ctx=4096,  # Contexte suffisamment grand
            n_gpu_layers=-1,  # Utiliser tous les layers GPU disponibles
            n_threads=4,  # Ajuster selon CPU
            verbose=False,
        )
        print("âœ… Nouveau modÃ¨le LLM initialisÃ©")

    def _add_message_safely(self, role, content):
        """Ajoute un message en vÃ©rifiant l'alternance des rÃ´les"""
        if not self.conversation_history:
            # Premier message (system)
            self.conversation_history.append({"role": role, "content": content})
            return

        last_role = self.conversation_history[-1]["role"]

        # VÃ©rifier l'alternance (sauf pour system au dÃ©but)
        if last_role == role and role != "system":
            print(f"âš ï¸ Tentative d'ajout de deux messages {role} consÃ©cutifs - ignorÃ©")
            return

        self.conversation_history.append({"role": role, "content": content})

    def get_next_decision(self):
        """Obtient la prochaine dÃ©cision du DJ IA"""

        # Mise Ã  jour du temps de session
        current_time = time.time()
        if self.session_state.get("last_action_time", 0) > 0:
            elapsed = current_time - self.session_state["last_action_time"]
            self.session_state["session_duration"] = (
                self.session_state.get("session_duration", 0) + elapsed
            )
        self.session_state["last_action_time"] = current_time

        # Construire le prompt utilisateur
        user_prompt = self._build_prompt()

        # Ajouter Ã  l'historique de conversation
        self._add_message_safely("user", user_prompt)

        if len(self.conversation_history) > 19:

            system_prompt = self.conversation_history[0]
            recent_pairs = self.conversation_history[-16:]

            self.conversation_history = [system_prompt] + recent_pairs

        print(
            f"\nğŸ§  GÃ©nÃ©ration AI-DJ avec {len(self.conversation_history)} messages d'historique..."
        )

        # GÃ©nÃ©rer avec tout l'historique
        response = self.model.create_chat_completion(self.conversation_history)

        print("âœ… GÃ©nÃ©ration terminÃ©e !")

        try:
            # Extraire et parser la rÃ©ponse JSON
            response_text = response["choices"][0]["message"]["content"]

            # Ajouter la rÃ©ponse Ã  l'historique AVANT de parser
            self._add_message_safely("assistant", response_text)

            # Trouver le JSON dans la rÃ©ponse
            import re

            json_match = re.search(r"({.*})", response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                decision = json.loads(json_str)
            else:
                # Fallback si pas de JSON trouvÃ©
                decision = {
                    "action_type": "generate_sample",
                    "parameters": {
                        "sample_details": {
                            "musicgen_prompt": "techno kick drum, driving beat",
                            "key": self.session_state.get("current_key", "C minor"),
                        }
                    },
                    "reasoning": "Fallback: Pas de rÃ©ponse JSON valide",
                }

        except (json.JSONDecodeError, KeyError) as e:
            print(f"Erreur de parsing de la rÃ©ponse: {e}")
            print(f"RÃ©ponse brute: {response_text}")

            # DÃ©cision par dÃ©faut en cas d'erreur
            decision = {
                "action_type": "generate_sample",
                "parameters": {
                    "sample_details": {
                        "musicgen_prompt": "electronic music sample",
                        "key": self.session_state.get("current_key", "C minor"),
                    }
                },
                "reasoning": f"Erreur de parsing: {str(e)}",
            }

        # Enregistrer aussi dans l'historique legacy (si besoin pour autres parties du code)
        if "history" not in self.session_state:
            self.session_state["history"] = []
        self.session_state["history"].append(decision)

        return decision

    def _build_prompt(self):
        """Prompt user avec prioritÃ© sur la demande actuelle"""
        user_prompt = self.session_state.get("user_prompt", "")
        current_tempo = self.session_state.get("current_tempo", 126)
        current_key = self.session_state.get("current_key", "C minor")

        return f"""âš ï¸ NOUVELLE DEMANDE UTILISATEUR âš ï¸
Mots-clÃ©s: {user_prompt}

Context:
- Tempo: {current_tempo} BPM  
- TonalitÃ©: {current_key}

IMPORTANT: Cette nouvelle demande est PRIORITAIRE. Si elle est diffÃ©rente de tes gÃ©nÃ©rations prÃ©cÃ©dentes, ABANDONNE complÃ¨tement le style prÃ©cÃ©dent et concentre-toi sur cette nouvelle demande."""

    def get_system_prompt(self) -> str:
        return """Tu es un gÃ©nÃ©rateur de samples musicaux intelligent. L'utilisateur te donne des mots-clÃ©s, tu gÃ©nÃ¨res un JSON cohÃ©rent.

FORMAT OBLIGATOIRE :
{
    "action_type": "generate_sample",
    "parameters": {
        "sample_details": {
            "musicgen_prompt": "[prompt optimisÃ© pour MusicGen basÃ© sur les mots-clÃ©s]",
            "key": "[tonalitÃ© appropriÃ©e ou garde celle fournie]"
        }
    },
    "reasoning": "Explication courte de tes choix"
}

RÃˆGLES DE PRIORITÃ‰ :
1. ğŸ”¥ SI l'utilisateur demande un style/genre spÃ©cifique â†’ IGNORE l'historique et gÃ©nÃ¨re exactement ce qu'il demande
2. ğŸ“ SI c'est une demande vague ou similaire â†’ Tu peux tenir compte de l'historique pour la variÃ©tÃ©
3. ğŸ¯ TOUJOURS respecter les mots-clÃ©s exacts de l'utilisateur

RÃˆGLES TECHNIQUES :
- CrÃ©e un prompt MusicGen cohÃ©rent et prÃ©cis
- Pour la tonalitÃ© : utilise celle fournie ou adapte si nÃ©cessaire
- RÃ©ponds UNIQUEMENT en JSON

EXEMPLES :
User: "deep techno rhythm kick hardcore" â†’ musicgen_prompt: "deep techno kick drum, hardcore rhythm, driving 4/4 beat, industrial"
User: "ambient space" â†’ musicgen_prompt: "ambient atmospheric space soundscape, ethereal pads"
User: "jazzy piano" â†’ musicgen_prompt: "jazz piano, smooth chords, melodic improvisation"
"""

    def reset_conversation(self):
        """Remet Ã  zÃ©ro l'historique de conversation"""
        self.conversation_history = [
            {"role": "system", "content": self.get_system_prompt()}
        ]
        print("ğŸ”„ Historique de conversation remis Ã  zÃ©ro")
